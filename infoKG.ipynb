{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceec02ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import torch\n",
    "from itertools import combinations\n",
    "from tqdm.notebook import tqdm\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import spacy\n",
    "from pyvis.network import Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0ea791a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Failed with utf-8: 'utf-8' codec can't decode byte 0x92 in position 113: invalid start byte\n",
      "‚úÖ Loaded & normalized 259 total documents.\n",
      "Columns: ['filename', 'subfolder', 'description', 'file_type', 'source']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>subfolder</th>\n",
       "      <th>description</th>\n",
       "      <th>file_type</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Amendment 2 to Bidding Documents</td>\n",
       "      <td>Tender Document.zip\\Tender Document\\Stage I\\Am...</td>\n",
       "      <td>This file introduces early modifications to th...</td>\n",
       "      <td>Tender Amendment/Clarification</td>\n",
       "      <td>Tender</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Thermac_Elec</td>\n",
       "      <td>Thermax Bid &amp; MOMs.zip\\Thermax Bid &amp; MOMs\\Post...</td>\n",
       "      <td>An Excel sheet containing electrical-related p...</td>\n",
       "      <td>Supporting Document</td>\n",
       "      <td>MOM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>Daily Progress Report TL FGD KTPS 20.04.23.xlsx</td>\n",
       "      <td>None</td>\n",
       "      <td>daily progress report of the date mentioned at...</td>\n",
       "      <td>General Project Document</td>\n",
       "      <td>DuringWork</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>TL DVC FGD WPR 31.03.23.xlsx</td>\n",
       "      <td>None</td>\n",
       "      <td>This file is a Weekly Project Report for the F...</td>\n",
       "      <td>Weekly Progress Report</td>\n",
       "      <td>DuringWork</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>Letter no- 157_DVC.pdf</td>\n",
       "      <td>None</td>\n",
       "      <td>This letter details major delays in FGD system...</td>\n",
       "      <td>Force Majeure Notice</td>\n",
       "      <td>DuringWork</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            filename  \\\n",
       "1                   Amendment 2 to Bidding Documents   \n",
       "42                                      Thermac_Elec   \n",
       "153  Daily Progress Report TL FGD KTPS 20.04.23.xlsx   \n",
       "252                     TL DVC FGD WPR 31.03.23.xlsx   \n",
       "226                           Letter no- 157_DVC.pdf   \n",
       "\n",
       "                                             subfolder  \\\n",
       "1    Tender Document.zip\\Tender Document\\Stage I\\Am...   \n",
       "42   Thermax Bid & MOMs.zip\\Thermax Bid & MOMs\\Post...   \n",
       "153                                               None   \n",
       "252                                               None   \n",
       "226                                               None   \n",
       "\n",
       "                                           description  \\\n",
       "1    This file introduces early modifications to th...   \n",
       "42   An Excel sheet containing electrical-related p...   \n",
       "153  daily progress report of the date mentioned at...   \n",
       "252  This file is a Weekly Project Report for the F...   \n",
       "226  This letter details major delays in FGD system...   \n",
       "\n",
       "                          file_type      source  \n",
       "1    Tender Amendment/Clarification      Tender  \n",
       "42              Supporting Document         MOM  \n",
       "153        General Project Document  DuringWork  \n",
       "252          Weekly Progress Report  DuringWork  \n",
       "226            Force Majeure Notice  DuringWork  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def load_csv(path):\n",
    "    \"\"\"Load CSV robustly with fallback encodings.\"\"\"\n",
    "    for enc in ['utf-8', 'latin1', 'ISO-8859-1']:\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc, on_bad_lines='skip')\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Failed with {enc}: {e}\")\n",
    "    return pd.read_csv(path, encoding='latin1', on_bad_lines='skip')\n",
    "\n",
    "# Load all CSVs\n",
    "tender_df = load_csv(\"tender.csv\")\n",
    "mom_df    = load_csv(\"mom.csv\")\n",
    "work_df   = load_csv(\"duringWork.csv\")\n",
    "\n",
    "# Clean and unify column names\n",
    "for df in [tender_df, mom_df, work_df]:\n",
    "    df.columns = [c.strip().lower() for c in df.columns]\n",
    "\n",
    "# Add missing 'subfolder' column if not present\n",
    "for df in [tender_df, mom_df, work_df]:\n",
    "    if 'subfolder' not in df.columns:\n",
    "        df['subfolder'] = None\n",
    "\n",
    "# Add dataset identifiers\n",
    "tender_df[\"source\"] = \"Tender\"\n",
    "mom_df[\"source\"]    = \"MOM\"\n",
    "work_df[\"source\"]   = \"DuringWork\"\n",
    "\n",
    "# Keep only required columns (fill missing automatically)\n",
    "common_cols = [\"filename\", \"subfolder\", \"description\", \"file_type\", \"source\"]\n",
    "tender_df = tender_df.reindex(columns=common_cols)\n",
    "mom_df    = mom_df.reindex(columns=common_cols)\n",
    "work_df   = work_df.reindex(columns=common_cols)\n",
    "\n",
    "# Merge all datasets\n",
    "docs = pd.concat([tender_df, mom_df, work_df], ignore_index=True)\n",
    "docs.dropna(subset=[\"description\"], inplace=True)\n",
    "docs.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(f\"‚úÖ Loaded & normalized {len(docs)} total documents.\")\n",
    "print(\"Columns:\", docs.columns.tolist())\n",
    "\n",
    "# Optional sanity check\n",
    "display(docs.sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "119331ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding documents with SentenceTransformer...\n"
     ]
    }
   ],
   "source": [
    "print(\"Encoding documents with SentenceTransformer...\")\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "docs[\"embedding\"] = docs[\"description\"].apply(lambda x: model.encode(str(x), convert_to_tensor=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cbfb0d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def extract_entities(text):\n",
    "    \"\"\"Extract project names, issues, dates, and months.\"\"\"\n",
    "    doc = nlp(text)\n",
    "    entities = set()\n",
    "\n",
    "    # Named entities (ORG, GPE, DATE, EVENT)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in [\"ORG\", \"GPE\", \"DATE\", \"EVENT\"]:\n",
    "            entities.add(ent.text.strip())\n",
    "\n",
    "    # Months\n",
    "    months = re.findall(r\"(January|February|March|April|May|June|July|August|September|October|November|December|Jan'?\\d{2,4}|Feb'?\\d{2,4}|Mar'?\\d{2,4}|Apr'?\\d{2,4}|May'?\\d{2,4}|Jun'?\\d{2,4}|Jul'?\\d{2,4}|Aug'?\\d{2,4}|Sep'?\\d{2,4}|Oct'?\\d{2,4}|Nov'?\\d{2,4}|Dec'?\\d{2,4})\", text, re.IGNORECASE)\n",
    "    entities.update([m.capitalize() for m in months])\n",
    "\n",
    "    # Issue/condition keywords\n",
    "    issue_keywords = [\"delay\", \"payment\", \"shortage\", \"strike\", \"rain\", \"monsoon\",\n",
    "                      \"lockdown\", \"covid\", \"extension\", \"hindrance\", \"force majeure\"]\n",
    "    for kw in issue_keywords:\n",
    "        if re.search(rf\"\\b{kw}\\b\", text, re.IGNORECASE):\n",
    "            entities.add(kw.capitalize())\n",
    "\n",
    "    return list(entities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "67a2ecbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Knowledge Graph...\n",
      "Extracting entities...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93c19be7fd074cf5b4bef3f6ae657c16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Building Knowledge Graph...\")\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add document nodes\n",
    "for _, row in docs.iterrows():\n",
    "    G.add_node(row[\"filename\"], type=row[\"source\"], desc=row[\"description\"])\n",
    "\n",
    "# Add extracted entity nodes\n",
    "print(\"Extracting entities...\")\n",
    "for _, row in tqdm(docs.iterrows(), total=len(docs)):\n",
    "    ents = extract_entities(row[\"description\"])\n",
    "    for ent in ents:\n",
    "        if not G.has_node(ent):\n",
    "            G.add_node(ent, type=\"Entity\")\n",
    "        G.add_edge(row[\"filename\"], ent, label=\"mentions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "562cfa0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing semantic similarities...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac099c7ce52649c1ba57d03f4b2451fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/33411 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Computing semantic similarities...\")\n",
    "threshold = 0.72  # tweak based on dataset size\n",
    "for i, j in tqdm(list(combinations(range(len(docs)), 2))):\n",
    "    sim = util.cos_sim(docs.iloc[i][\"embedding\"], docs.iloc[j][\"embedding\"]).item()\n",
    "    if sim > threshold:\n",
    "        G.add_edge(\n",
    "            docs.iloc[i][\"filename\"],\n",
    "            docs.iloc[j][\"filename\"],\n",
    "            label=f\"semantic_similarity ({sim:.2f})\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3c1e3b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding project and temporal links...\n"
     ]
    }
   ],
   "source": [
    "print(\"Adding project and temporal links...\")\n",
    "\n",
    "# Expanded project keyword list\n",
    "project_keywords = [\n",
    "    \"FGD\", \"Flue Gas Desulphurization\", \"DVC\", \"Damodar Valley\",\n",
    "    \"Koderma\", \"KTPS\", \"Thermax\", \"BHEL\", \"Unit\", \"Package\", \"Project\",\n",
    "    \"Power Station\", \"Thermal Power\", \"FGD Package\"\n",
    "]\n",
    "\n",
    "# Loop through all document descriptions\n",
    "for _, row in docs.iterrows():\n",
    "    text = row[\"description\"]\n",
    "\n",
    "    # Link months as Time nodes ---\n",
    "    months = re.findall(\n",
    "        r\"(January|February|March|April|May|June|July|August|September|October|November|December)\",\n",
    "        text, re.IGNORECASE\n",
    "    )\n",
    "    for m in months:\n",
    "        m = m.capitalize()\n",
    "        if not G.has_node(m):\n",
    "            G.add_node(m, type=\"Time\")\n",
    "        G.add_edge(row[\"filename\"], m, label=\"time_related\")\n",
    "\n",
    "    # Link explicit project keywords ---\n",
    "    for p in project_keywords:\n",
    "        if re.search(rf\"\\b{p}\\b\", text, re.IGNORECASE):\n",
    "            if not G.has_node(p):\n",
    "                G.add_node(p, type=\"Project\")\n",
    "            G.add_edge(row[\"filename\"], p, label=\"related_to_project\")\n",
    "\n",
    "    #Use NLP to auto-detect project/org names ---\n",
    "    doc_nlp = nlp(text)\n",
    "    for ent in doc_nlp.ents:\n",
    "        if ent.label_ in [\"ORG\", \"FAC\", \"GPE\"]:\n",
    "            ent_text = ent.text.strip()\n",
    "            # filter out very short or generic names\n",
    "            if len(ent_text) > 3 and not ent_text.isdigit():\n",
    "                if not G.has_node(ent_text):\n",
    "                    G.add_node(ent_text, type=\"Project\")\n",
    "                G.add_edge(row[\"filename\"], ent_text, label=\"related_to_project\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0b9191ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding semantic concept links (Delay, Payment, Shortage, etc.)...\n",
      "‚úÖ Semantic concept linking complete!\n"
     ]
    }
   ],
   "source": [
    "print(\"Adding semantic concept links (Delay, Payment, Shortage, etc.)...\")\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# load model once (you can reuse if already loaded earlier)\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# define concept themes\n",
    "concepts = {\n",
    "    \"Delay\": \"project delay, work hindrance, obstruction, slow progress, stoppage, lockdown, strike, force majeure, covid disruption\",\n",
    "    \"Payment Issue\": \"delayed payment, fund shortage, pending bills, financial stress, cash flow issue, non-payment\",\n",
    "    \"Material Shortage\": \"shortage of sand, cement, aggregates, material supply issue, delay due to materials\",\n",
    "    \"Manpower Issue\": \"lack of workers, labour shortage, gate pass delay, manpower unavailability, staffing issue\"\n",
    "}\n",
    "\n",
    "# encode concept vectors\n",
    "concept_embeddings = {c: model.encode(desc, convert_to_tensor=True) for c, desc in concepts.items()}\n",
    "\n",
    "# compare each document with each concept\n",
    "for _, row in docs.iterrows():\n",
    "    desc = str(row[\"description\"])\n",
    "    emb = model.encode(desc, convert_to_tensor=True)\n",
    "\n",
    "    for concept, concept_emb in concept_embeddings.items():\n",
    "        sim = util.cos_sim(concept_emb, emb).item()\n",
    "        if sim > 0.45:  # threshold for semantic similarity\n",
    "            if not G.has_node(concept):\n",
    "                G.add_node(concept, type=\"Concept\")\n",
    "            G.add_edge(row[\"filename\"], concept, label=\"semantically_related\")\n",
    "\n",
    "print(\"‚úÖ Semantic concept linking complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "15bf59cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Nodes: 405\n",
      "Total Edges: 2895\n",
      "Node types: Counter({'DuringWork': 142, 'Entity': 138, 'MOM': 79, 'Tender': 34, 'Project': 9, 'Concept': 3})\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Nodes:\", G.number_of_nodes())\n",
    "print(\"Total Edges:\", G.number_of_edges())\n",
    "\n",
    "# Count node types\n",
    "from collections import Counter\n",
    "types = Counter(nx.get_node_attributes(G, \"type\").values())\n",
    "print(\"Node types:\", types)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3a5333f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project nodes: ['Package', 'FGD Package', 'Project', 'Unit', 'Damodar Valley', 'Koderma', 'Power Station', 'Thermal Power', 'Flue Gas Desulphurization']\n",
      "\n",
      "Documents connected to project:\n",
      " - Amendment 2 to Bidding Documents (Tender)\n",
      " - AMENDMENT-KTPS (Tender)\n",
      " - KTPS-PART A-1 (Tender)\n",
      " - KTPS-PART D (Tender)\n",
      " - KTPS-PART F (Tender)\n",
      " - SECTION VII (Tender)\n",
      " - Amendment 8 to Bidding Documents (Tender)\n",
      " - Amendment 9 to Bidding Documents (Tender)\n",
      " - Stage-II Bidding Documents (Tender)\n",
      " - Final Post_Bid_Tech & Comm. MOM_DVC Bulk (MOM)\n",
      " - MoM_QA_DVC_Thermac (MOM)\n",
      " - Thermac_Main (MOM)\n",
      " - Attachment 3K_Annexure 6_Oxidation Blowers_Aerzen Machines (MOM)\n",
      " - Attachment 3K_Annexure 17_VBF_Eimco_KCP (MOM)\n",
      " - 1_BID FORM (MOM)\n",
      " - Attachment 20 (MOM)\n",
      " - DVC-01_MPR Oct'2019.pdf (DuringWork)\n",
      " - DVC-30_MPR MAY'2022.pdf (DuringWork)\n",
      " - DVC-31_MPR JUN'2022.pdf (DuringWork)\n",
      " - DVC-32_MPR JUL'2022.pdf (DuringWork)\n",
      " - DVC-33_MPR Aug'2022.pdf (DuringWork)\n",
      " - DVC-34_MPR Sep'2022.pdf (DuringWork)\n",
      " - DVC-35_MPR Oct'2022. pdf (DuringWork)\n",
      " - DVC-36_MPR Nov'2022.pdf (DuringWork)\n",
      " - DVC-37_MPR Dec'2022.pdf (DuringWork)\n",
      " - DVC-38_MPR Jan'2023.pdf (DuringWork)\n",
      " - DVC-39_MPR Feb'2023.pdf (DuringWork)\n",
      " - DVC-40_MPR Mar'2023.pdf (DuringWork)\n",
      " - DVC-41_MPR Apr'2023.pdf (DuringWork)\n",
      " - DVC-42_MPR May'2023.pdf (DuringWork)\n",
      " - DVC-43_MPR Jun'2023.pdf (DuringWork)\n",
      " - DVC-44_MPR Jul'2023.pdf (DuringWork)\n",
      " - DVC-45_MPR Aug'2023.pdf (DuringWork)\n",
      " - DVC-46_MPR Sep'2023.pdf (DuringWork)\n",
      " - DVC-47_MPR Oct'2023.pdf (DuringWork)\n",
      " - DVC-48_MPR Nov'2023.pdf (DuringWork)\n",
      " - DVC-49_MPR Dec'2023.pdf (DuringWork)\n",
      " - DVC-50_MPR Jan'2024.pdf (DuringWork)\n",
      " - DVC-51_MPR Feb'2024.pdf (DuringWork)\n",
      " - DVC-52_MPR Mar'2024.pdf (DuringWork)\n",
      " - DVC-53_MPR Apr'2024.pdf (DuringWork)\n",
      " - DVC-54_MPR May'2024.pdf (DuringWork)\n",
      " - DVC-55_MPR Jun'2024.pdf (DuringWork)\n",
      " - DVC-56_MPR Jul'2024.pdf (DuringWork)\n",
      " - DVC-57_MPR Aug'2024.pdf (DuringWork)\n",
      " - DVC-58_MPR Sep'2024.pdf (DuringWork)\n",
      " - DVC-59_MPR Oct'2024.pdf (DuringWork)\n",
      " - DVC-60_MPR Nov'2024.pdf (DuringWork)\n",
      " - DVC-61_MPR Dec'2024.pdf (DuringWork)\n",
      " - DVC-62_MPR Jan'2025.pdf (DuringWork)\n"
     ]
    }
   ],
   "source": [
    "projects = [n for n, d in G.nodes(data=True) if d.get(\"type\") == \"Project\"]\n",
    "print(\"Project nodes:\", projects)\n",
    "if projects:\n",
    "    print(\"\\nDocuments connected to project:\")\n",
    "    for nbr in G.neighbors(projects[0]):\n",
    "        print(f\" - {nbr} ({G.nodes[nbr]['type']})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ea9acde8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shortest path between 'Damodar Valley' and 'FGD Package':\n",
      "Damodar Valley ‚Üí DVC-01_MPR Oct'2019.pdf ‚Üí FGD ‚Üí Amendment 2 to Bidding Documents ‚Üí FGD Package\n"
     ]
    }
   ],
   "source": [
    "print(\"Shortest path between 'Damodar Valley' and 'FGD Package':\")\n",
    "try:\n",
    "    path = nx.shortest_path(G, \"Damodar Valley\", \"FGD Package\")\n",
    "    print(\" ‚Üí \".join(path))\n",
    "except nx.NetworkXNoPath:\n",
    "    print(\"No path found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778cf796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìò Documents connected to 'Delay':\n",
      " - Lockdown.pdf (DuringWork)\n",
      " - PM Letters.pdf (DuringWork)\n",
      " - Request for Issue of Form-lll for SIPPL.pdf (DuringWork)\n",
      " - 01 - Force Majeure Covid 19.pdf (DuringWork)\n",
      " - 02 - Force Majeure Covid 19.pdf (DuringWork)\n",
      " - Letter no- 159_DVC.pdf (DuringWork)\n"
     ]
    }
   ],
   "source": [
    "entity = \"Delay\"\n",
    "if entity in G.nodes:\n",
    "    print(f\"Documents connected to '{entity}':\")\n",
    "    for nbr in G.neighbors(entity):\n",
    "        print(f\" - {nbr} ({G.nodes[nbr]['type']})\")\n",
    "else:\n",
    "    print(f\"'{entity}' not found in graph.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4b349048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic edges count: 1678\n",
      "Sample: [('Amendment 1 to NIT', 'Amendment 2 to Bidding Documents', 'semantic_similarity (0.80)'), ('Amendment 1 to NIT', 'CLARIFICATION ON BIDDING DOCUMENTS_20122018', 'semantic_similarity (0.74)'), ('Amendment 1 to NIT', 'Amendment 3 to NIT', 'semantic_similarity (0.83)'), ('Amendment 1 to NIT', 'Reply to All Pre Bid Queries (Technical) - Bulk Tender', 'semantic_similarity (0.74)'), ('Amendment 1 to NIT', 'Amendment No. 5', 'semantic_similarity (0.83)')]\n"
     ]
    }
   ],
   "source": [
    "semantic_edges = [(u, v, d[\"label\"]) for u, v, d in G.edges(data=True) if \"semantic_similarity\" in d[\"label\"]]\n",
    "print(f\"Semantic edges count: {len(semantic_edges)}\")\n",
    "print(\"Sample:\", semantic_edges[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e353175c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected Components: 29\n",
      "Largest Component Size: 355 nodes\n"
     ]
    }
   ],
   "source": [
    "components = list(nx.connected_components(G))\n",
    "print(f\"Connected Components: {len(components)}\")\n",
    "largest = max(components, key=len)\n",
    "print(f\"Largest Component Size: {len(largest)} nodes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f086a756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rendering interactive Knowledge Graph...\n",
      "Warning: When  cdn_resources is 'local' jupyter notebook has issues displaying graphics on chrome/safari. Use cdn_resources='in_line' or cdn_resources='remote' if you have issues viewing graphics in a notebook.\n",
      "knowledge_graph.html\n",
      "‚úÖ Graph saved as knowledge_graph.html ‚Äî open it in browser.\n"
     ]
    }
   ],
   "source": [
    "print(\"Rendering interactive Knowledge Graph...\")\n",
    "\n",
    "net = Network(height=\"750px\", width=\"100%\", notebook=True,\n",
    "              bgcolor=\"#ffffff\", font_color=\"black\", directed=False)\n",
    "net.barnes_hut()\n",
    "\n",
    "color_map = {\n",
    "    \"Tender\": \"#87CEEB\",      # blue\n",
    "    \"DuringWork\": \"#90EE90\",  # green\n",
    "    \"MOM\": \"#FFD700\",         # yellow\n",
    "    \"Entity\": \"#FF7F7F\",      # red\n",
    "    \"Project\": \"#FFA500\",     # orange\n",
    "    \"Time\": \"#9370DB\"         # purple\n",
    "}\n",
    "\n",
    "for node, data in G.nodes(data=True):\n",
    "    color = color_map.get(data.get(\"type\", \"\"), \"#D3D3D3\")\n",
    "    net.add_node(node, label=node, color=color, title=data.get(\"desc\", \"\"))\n",
    "\n",
    "for src, dst, data in G.edges(data=True):\n",
    "    net.add_edge(src, dst, title=data.get(\"label\", \"\"))\n",
    "\n",
    "net.show(\"knowledge_graph.html\")\n",
    "print(\"‚úÖ Graph saved as knowledge_graph.html ‚Äî open it in browser.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "df89bc2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 259 document embeddings.\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# store embeddings for retrieval\n",
    "doc_embeddings = []\n",
    "for _, row in docs.iterrows():\n",
    "    emb = model.encode(str(row[\"description\"]), convert_to_tensor=True)\n",
    "    doc_embeddings.append((row[\"filename\"], emb, row[\"source\"]))\n",
    "\n",
    "print(f\"Stored {len(doc_embeddings)} document embeddings.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7e938455",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_retrieve(query, top_k=5):\n",
    "    q_emb = model.encode(query, convert_to_tensor=True)\n",
    "    results = []\n",
    "    for name, emb, source in doc_embeddings:\n",
    "        sim = util.cos_sim(q_emb, emb).item()\n",
    "        results.append((name, sim, source))\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    return results[:top_k]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "90c3f6e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Letter no- 160_DVC.pdf', 0.42702797055244446, 'DuringWork'),\n",
       " ('Delay in Issuing of Gate-Passes of Workers.pdf',\n",
       "  0.4032321572303772,\n",
       "  'DuringWork'),\n",
       " ('Letter no- 157_DVC.pdf', 0.39488762617111206, 'DuringWork'),\n",
       " ('Regarding Delay in Vendor Approval Samal Infra Projects Pvt. Ltd..pdf',\n",
       "  0.3912941813468933,\n",
       "  'DuringWork'),\n",
       " ('01 - Force Majeure Covid 19.pdf', 0.38478004932403564, 'DuringWork')]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semantic_retrieve(\"reasons for work delay in August\", top_k=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8a54062c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Knowledge graph exported to 'knowledge_graph.graphml'\n"
     ]
    }
   ],
   "source": [
    "nx.write_graphml(G, \"knowledge_graph.graphml\")\n",
    "\n",
    "print(\"‚úÖ Knowledge graph exported to 'knowledge_graph.graphml'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "KnowledgeGraph-82NpGr-d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
